- ANN의 기본 구조
- 인공 신경망의 선형 및 비선형 구성 요소
- 신경망과 관련된 여러 핵심 용어

----
## The perceptron : 선형모델의 경우

![39.Pasted image 20241021075933](../pic/7.%20ANNs/39.Pasted%20image%2020241021075933.png)

가중평균: 가중합/$\sum W_i$
	가중치의 합이 1이라면 가중합과 가중평균은 동일

![39.Pasted image 20241021080122](../pic/7.%20ANNs/39.Pasted%20image%2020241021080122.png)

가중합: 내적

---
## Linear vs. nonlinear operations

<span style="color:rgb(118, 147, 234)">Linear:</span> <span style="color:rgb(116, 195, 194)">더하기, 곱하기</span>
<span style="color:rgb(236, 158, 111)">Nonlinear:</span> <span style="color:rgb(205, 205, 81)">모든 연산</span> 

<span style="color:rgb(118, 147, 234)">선형 모델은 오직 선형으로 분리 가능한 문제만 해결한다.</span> 
<span style="color:rgb(236, 158, 111)">비선형 모델은 좀더 복잡한 문제를 해결할 수 있다.</span> 

<span style="color:rgb(230, 122, 122)">선형 모델을 비선형 문제에 절대 사용하지 말고,</span>
<span style="color:rgb(230, 122, 122)">비선형 모델을 선형 문제에 절대 사용하지 마라!</span> 

---
## The perceptron : 비선형 모델의 경우

![39.Pasted image 20241021081131](../pic/7.%20ANNs/39.Pasted%20image%2020241021081131.png)

![39.Pasted image 20241021081640](../pic/7.%20ANNs/39.Pasted%20image%2020241021081640.png)

![39.Pasted image 20241021081745](../pic/7.%20ANNs/39.Pasted%20image%2020241021081745.png)

	부호 함수(sign) : 각각의 값이 무엇인지보다는, 전체 값이 0보다 크거나 작은지가 중요

	이러한 비선형 함수(sigma)를 활성함수(activation function)이라고 부름

부호 함수 그래프
![39.Pasted image 20241021081400](../pic/7.%20ANNs/39.Pasted%20image%2020241021081400.png)

---
## The math of deep learning

![39.Pasted image 20241021082214](../pic/7.%20ANNs/39.Pasted%20image%2020241021082214.png)

---
## The bias term(aka intercept)

<span style="color:rgb(205, 205, 81)">목표: 두가지 색을 분리하라</span> 

![39.Pasted image 20241021082659](../pic/7.%20ANNs/39.Pasted%20image%2020241021082659.png)

	절편이 없더라도 위와같은 그래프는 분리가 가능

![39.Pasted image 20241021082932](../pic/7.%20ANNs/39.Pasted%20image%2020241021082932.png)

	하지만 위와 같은 그래프는 분리가 불가능함 -> 절편이 없어 y=0부터 시작하기 때문

![39.Pasted image 20241021082943](../pic/7.%20ANNs/39.Pasted%20image%2020241021082943.png)

	절편을 도입하여 y값의 시작 위치를 변경하여 분리

---
## Do you need the bias term?

![39.Pasted image 20241021085022](../pic/7.%20ANNs/39.Pasted%20image%2020241021085022.png)

	데이터를 평균 중심화하도록 변환하여 절편없이도 가능하기는 함
	실제로는 항상 절편을 포하하는것이 좋음

	절편이 필요한 이유 : 원점을 지나도록 제한된 직선 때문에 최적이 아닌 결과가 나오는 상황을 피하기 위해 

---

## The perceptron with bias term

![40.Pasted image 20241021085709](../pic/7.%20ANNs/40.Pasted%20image%2020241021085709.png)

![40.Pasted image 20241021085734](../pic/7.%20ANNs/40.Pasted%20image%2020241021085734.png)

![40.Pasted image 20241021090031](../pic/7.%20ANNs/40.Pasted%20image%2020241021090031.png)

	실질적으로는 W_0 가중치가 절편에 해당함
