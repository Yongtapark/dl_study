- 매개변수 실험을 설정하고 실행하는 방법 학습
- 경사 하강법에서 초기값, 학습률, 훈련 반복 횟수의 중요성을 이해
- 경사 하강법이 훌륭한 알고리즘이지만 정답을 보장하지 않는 사실을 재확인
---
## Our experiments

<span style="color:rgb(118, 147, 234)">1D 모델을 반복</span>

<span style="color:rgb(116, 195, 194)">단일 변수 실험:</span>
1) <span style="color:rgb(116, 195, 194)">초기값</span>
2) <span style="color:rgb(116, 195, 194)">학습률</span>

<span style="color:rgb(205, 205, 81)">이중 변수 실험:
</span>
1) <span style="color:rgb(205, 205, 81)">학습률과 트레이닝 에포크</span> 
---

## Code

```python
# import all necessary modules
import numpy as np
import matplotlib.pyplot as plt

from IPython import display
display.set_matplotlib_formats('svg')
```

## Running experiments to understand gradient descent
```python
# the function
x = np.linspace(-2*np.pi,2*np.pi,401)
fx = np.sin(x) * np.exp(-x**2*.05)

# and its derivative
df = np.cos(x)*np.exp(-x**2*.05)+np.sin(x)*(-.1*x)*np.exp(-x**2*.05)

# quick plot for inspection
plt.plot(x,fx,x,df)
plt.legend(['f(x)','df'])
plt.show()
```
![](36.Pasted%20image%2020241019143648.png)

```python
# function (note: over-writing variable names!)
def fx(x):
    return np.sin(x)*np.exp(-x**2*.05)

# derivative function
def deriv(x):
    return np.cos(x)*np.exp(-x**2*.05)-np.sin(x)*.1*np.exp(-x**2*.05)
```

```python
# random starting point
localmin = np.random.choice(x,1)#np.array([6])

# learning parameters
learning_rate = .01
training_epochs = 1000

# run through training
for i in range(training_epochs):
    grad = deriv(localmin)
    localmin = localmin-learning_rate*grad


# plot the results
plt.plot(x,fx(x),x,deriv(x),'--')
plt.plot(localmin,deriv(localmin),'ro')
plt.plot(localmin,fx(localmin),'ro')

plt.xlim(x[[0,-1]])
plt.grid()
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend(['f(x)','df','f(x) min'])
plt.title('Empirical local minimum: %s'%localmin[0])
plt.show()
```
![](36.Pasted%20image%2020241019143706.png)
## Run parametric experiments
```python
# Experiment 1: systematically varying the starting location

startlocs = np.linspace(-5,5,50)
finalres = np.zeros(len(startlocs))

# loop over starting points
for idx,localmin in enumerate(startlocs):

    # run through training
    for i in range(training_epochs):
        grad = deriv(localmin)
        localmin = localmin - grad* learning_rate

    # store the final guess
    finalres[idx] = localmin

# plot the result
plt.plot(startlocs,finalres,'s-')
plt.xlabel('Starting guess')

# 로컬 미니마의 위치를 다른 그래프로 플로팅
```
![](36.Pasted%20image%2020241019143729.png)

```python
# Experiment 2: systematically varying the learning rate

learningrates = np.linspace(1e-10,1e-1,50)
finalres = np.zeros(len(learningrates))

# loop over learning rates
for idx,learningrate in enumerate(learningrates):

    # force starting guess to 0
    localmin = 0

    # run through training
    for i in range(training_epochs):
        grad = deriv(localmin)
        localmin = localmin - learningrate*grad

    # store the final guess
    finalres[idx] = localmin

plt.plot(learningrates,finalres,'s-')
plt.xlabel('Learning rate')
plt.ylabel('Final guess')
plt.show()

# 학습률이 조금만 커져도, 매우 빠르게 최적의 결과에 도달
# 이 함수의 데이터셋은 학습률에 비교적 강건함

# 하지만 일정 학습률 범위에서는 모델이 학습하지 않거나, 불안정하고 혼란스러운 결과를 도출하는 경우가 자주있음
# 모델이 제대로 작동하는 학습 범위도 존재함
```
![](36.Pasted%20image%2020241019143800.png)

```python
# Experiment 3: interaction between learning rate and training epochs

# setup parameters
learningrates = np.linspace(1e-10,1e-1,50)
training_epochs = np.round(np.linspace(10,500,40))

# initialize matrix to stroe results
finalres = np.zeros((len(learningrates),len(training_epochs)))

# loop over learning rates
for Lidx,learningRate in enumerate(learningrates):

    # loop over training epochs
    for Eidx,trainEpochs in enumerate(training_epochs):

        # run through training (again fixing starting location)
        localmin = 0
        for i in range(int(trainEpochs)):
            grad = deriv(localmin)
            localmin = localmin - learningRate*grad

        # store the final guess
        finalres[Lidx,Eidx] = localmin

```

```python
# plot the results

fig,ax = plt.subplots(figsize=(7,5))

plt.imshow(finalres,extent=[learningrates[0],learningrates[-1],training_epochs[0],training_epochs[-1]],
           aspect='auto',origin='lower',vmin=-1.45,vmax=-1.2)
plt.xlabel('Learning rate')
plt.ylabel('Training epochs')
plt.title('Final guess')
plt.colorbar()
plt.show()

# another visualization
# 에포크를 고정하고 러닝레이트에 따라서 얼마나 빠르게 해답에 도달하는지
plt.plot(learningrates, finalres)
plt.xlabel('Learning rates')
plt.ylabel('Final function estimate')
plt.title('Each line is a training epochs N')
plt.show()

# 글로벌 미니마에 수렴될것이 확실한 위치 0에서 시작하여 러닝레이트와 에포크사이의 관계를 실험
```
![](36.Pasted%20image%2020241019143820.png)
![](Pasted%20image%2020241019143833.png)

## Additional explorations
```python
# 1) 실험 3에서 시작 위치를 1.6으로 설정하세요. 실험과 이미지를 다시 실행합니다. 
#    그림의 색상 제한을 다시 조정해야 할 수 있습니다. 코드 상단의 선형 그래프를 확인하여 적절한 색상 범위를 결정하세요. 
#    새로운 시작 값이 학습률(learning rate)과 훈련 에포크(training epochs) 간의 상호작용에 대한 결론을 바꾸나요?

# 2) 동일한 실험에서 이번에는 시작 위치를 무작위로 변경하세요 (코드: np.random.choice(x, 1) 사용). 
#    이러한 결과는 어떻게 보이나요? 놀랍습니까? 이 실험의 결과가 여전히 해석 가능한가요? 
#    그리고 이러한 결과가 딥러닝(DL) 실험을 수행하는 데 대해 무엇을 시사하나요?
```

## Question1

```python
# Me : 글로벌 미니마가 아닌 로컬 미니마에 수렴됨.

# setup parameters
learningrates = np.linspace(1e-10,1e-1,50)
training_epochs = np.round(np.linspace(10,500,40))

# initialize matrix to stroe results
finalres = np.zeros((len(learningrates),len(training_epochs)))

# loop over learning rates
for Lidx,learningRate in enumerate(learningrates):

    # loop over training epochs
    for Eidx,trainEpochs in enumerate(training_epochs):

        # run through training (again fixing starting location)
        localmin = 1.6
        for i in range(int(trainEpochs)):
            grad = deriv(localmin)
            localmin = localmin - learningRate*grad

        # store the final guess
        finalres[Lidx,Eidx] = localmin

# plot the results

fig,ax = plt.subplots(figsize=(7,5))

plt.imshow(finalres,extent=[learningrates[0],learningrates[-1],training_epochs[0],training_epochs[-1]],
           aspect='auto',origin='lower',vmin=-2,vmax=7)
plt.xlabel('Learning rate')
plt.ylabel('Training epochs')
plt.title('Final guess')
plt.colorbar()
plt.show()

# another visualization
# 에포크를 고정하고 러닝레이트에 따라서 얼마나 빠르게 해답에 도달하는지
plt.plot(learningrates, finalres)
plt.xlabel('Learning rates')
plt.ylabel('Final function estimate')
plt.title('Each line is a training epochs N')
plt.show()

# 글로벌 미니마에 수렴될것이 확실한 위치 0에서 시작하여 러닝레이트와 에포크사이의 관계를 실험
```
![](36.Pasted%20image%2020241019143853.png)
![](36.Pasted%20image%2020241019143908.png)

## Question 2

```python
# Me : 
# 1. global minima를 잘 찾을 수 있는 특정한 learning_rate와 training_epoch 구간이 존재한다.
# 2. 두 파라미터는 최적의 해를 찾기 위해 상호 균형이 중요하다.
# 3. 무작위 초기화에 따라 결과가 달라질 수 있으므로 여러 초기 조건에서 일관된 성능을 확인하는 것이 중요하다.

x = np.linspace(-2*np.pi,2*np.pi,401)

# setup parameters
learningrates = np.linspace(1e-10,1e-1,50)
training_epochs = np.round(np.linspace(10,500,40))

# initialize matrix to stroe results
finalres = np.zeros((len(learningrates),len(training_epochs)))

# loop over learning rates
for Lidx,learningRate in enumerate(learningrates):

    # loop over training epochs
    for Eidx,trainEpochs in enumerate(training_epochs):

        # run through training (again fixing starting location)
        localmin = np.random.choice(x,1)
        for i in range(int(trainEpochs)):
            grad = deriv(localmin)
            localmin = localmin - learningRate*grad

        # store the final guess
        finalres[Lidx,Eidx] = localmin

# plot the results

fig,ax = plt.subplots(figsize=(7,5))

plt.imshow(finalres,extent=[learningrates[0],learningrates[-1],training_epochs[0],training_epochs[-1]],
           aspect='auto',origin='lower',vmin=-1.45,vmax=-1.2)
plt.xlabel('Learning rate')
plt.ylabel('Training epochs')
plt.title('Final guess')
plt.colorbar()
plt.show()

# another visualization
# 에포크를 고정하고 러닝레이트에 따라서 얼마나 빠르게 해답에 도달하는지
plt.plot(learningrates, finalres)
plt.xlabel('Learning rates')
plt.ylabel('Final function estimate')
plt.title('Each line is a training epochs N')
plt.show()
```
![](36.Pasted%20image%2020241019143926.png)
![](36.Pasted%20image%2020241019143952.png)

.5로 범위 변경
![](36.Pasted%20image%2020241019150118.png)
