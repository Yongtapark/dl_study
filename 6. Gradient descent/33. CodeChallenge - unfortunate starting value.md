- 함수와 그 도함수를 작성하는 데 있어 더 많은 경험을 쌓기
- 시작값이 정확히 잘못되었을 때 무슨 일이 일어나는지 확인

---
## Goals for this code challenge

<span style="color:rgb(118, 147, 234)">Step 1: 1차원 경사하강법을 반복하여 다음 함수의 최소값을 찾아라:</span>

![33.Pasted image 20241014205550](../pic/6.%20Gradient%20descent/33.Pasted%20image%2020241014205550.png)

<span style="color:rgb(118, 147, 234)">Step 2: 시작값으로 x=0을 하드코딩하라</span> 

---
## Code

```python
import numpy as np
import matplotlib.pyplot as plt

from IPython import display
display.set_matplotlib_formats('svg')
```

```python
# function (as a function)
def fx(x):
    return np.cos(2*np.pi*x)+(x**2)

# derivate funtion
def deriv(x):
    return -(2*np.pi)*np.sin(2*np.pi*x)+(2*x)

```

```python
# plot the function and its derivative

# define a range for x
x = np.linspace(-2,2,2001)

# plotting
plt.plot(x,fx(x),x,deriv(x))
plt.xlim(x[[0,-1]])
plt.grid()
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend(['y','dy'])
plt.show()
```
![](33.Pasted%20image%2020241015211731.png)

```python
# random starting point
localmin = np.random.choice(x,1)
print(localmin)

# learning parameters
learning_rate = .01
training_epochs = 100

# run through training
for i in range(training_epochs):
    grad = deriv(localmin)
    localmin = localmin - learning_rate*grad

localmin
```

```python
plt.plot(x,fx(x),x,deriv(x))
plt.plot(localmin,deriv(localmin),'ro')
plt.plot(localmin,fx(localmin),'ro')

plt.xlim(x[[0,-1]])
plt.grid()
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend(['f(x)','df','f(x) min'])
plt.title('Empirical local minimum: %s'%localmin[0])
plt.show()
```
![](33.Pasted%20image%2020241015211749.png)
```python
# random starting point
localmin = np.array([0])
print(localmin)

# learning parameters
learning_rate = .01
training_epochs = 100

# run through training
for i in range(training_epochs):
    grad = deriv(localmin)
    localmin = localmin - learning_rate*grad


plt.plot(x,fx(x),x,deriv(x))
plt.plot(localmin,deriv(localmin),'ro')
plt.plot(localmin,fx(localmin),'ro')

plt.xlim(x[[0,-1]])
plt.grid()
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend(['f(x)','df','f(x) min'])
plt.title('Empirical local minimum: %s'%localmin[0])
plt.show()

# 기울기 소실 때문. 함수의 변화가 없는 구간이 존재한다.
# 기울기를 결정하는 공식은 다음과 같은데
# for i in range(training_epochs):
#    grad = deriv(localmin)
#    localmin = localmin - learning_rate*grad

# 함수 공식(-(2*np.pi)*np.sin(2*np.pi*x)+(2*x))에서도 x=0일때 값이 0이므로 기울기가 업데이트 되지 않음
```
![](33.Pasted%20image%2020241015211828.png)

```python
# 1.	도함수에 2라는 곱셈 상수가 포함되어 있습니다. 그 상수는 경사 하강법(g.d.) 결과의 정확성을 위해 필수적인가요? 
#       그 도함수에서 ‘2’를 제거하고 모델이 여전히 최솟값을 찾을 수 있는지 확인해보세요.
#       코드를 실행하기 전에, 어떤 결과를 예상하는지 생각해보세요. 현실이 예상과 일치하나요?
#       그 상수가 필수적인 이유는 무엇인가요, 또는 필수적이지 않은 이유는 무엇인가요?

# me : 모든 2 상수가 제거되면 도함수의 파동이 간격이 넓어지면서 지역최소값에 도달하지 못함. 
#      모든 2 상수를 제거하는 대신 기존 함수 내의 상수(2*np.pi*x)만 변경하지 않는다면 최소값에 어느정도 근사하게 접근함

# 2.	np.sin() 함수 안에 있는 ‘2’라는 상수는 중요한가요?
#       그것을 제거해도 정확한 결과를 얻을 수 있나요?

# me : 매우 중요함. 실험 결과 파동의 간격이 정해지는 부분인것으 보여짐. 제거하게되면 파동간격이 넓어져 제대로된 지역최소값을 얻기 힘듬

# 3.	초기 값을 작지만 0이 아닌 값으로 설정해보세요, 예를 들어 0.0001 또는 -0.0001로 설정해보세요.
#       그것이 해결에 도움이 되나요?

# me : 실험결과 0.0000001 까지는 지역최소값에 도달하며, 0.00000001은 지역최소값과 지역최대값의 중간부분,
#      0.000000001 부터는 기울기소실이 일어난다.
```