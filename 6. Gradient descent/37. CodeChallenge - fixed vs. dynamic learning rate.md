- 모델의 학습율이 반드시 상수일 필요가 없다는 것을 학습
- 학습율을 변화시키는 다른 방법과 특정 방법이 실패할 수 있는 경우
---
## Your mission

<span style="color:rgb(116, 195, 194)">학습률을 변경하는 방법에 대해 생각하라</span>
1) <span style="color:rgb(116, 195, 194)">시간(훈련 에포크)</span>
2) <span style="color:rgb(116, 195, 194)">도함수</span>
3) <span style="color:rgb(116, 195, 194)"> 손실</span>
4) <span style="color:rgb(116, 195, 194)">현재 지역 치소값</span>

<span style="color:rgb(205, 205, 81)">코드로 아이디어를 구현하고 테스트</span>

<span style="color:rgb(236, 158, 111)">코드가 실패할 수 있는 부분에 대해 비판적으로 생각</span>

---
## Code

```python
# import all necessary modules
import numpy as np
import matplotlib.pyplot as plt

import matplotlib_inline.backend_inline
matplotlib_inline.backend_inline.set_matplotlib_formats('svg')
```

```python
# function (as a function)
def fx(x):
  return 3*x**2 - 3*x + 4

# derivative function
def deriv(x):
  return 6*x - 3
```

```python
# plot the function and its derivative

# define a range for x
x = np.linspace(-2,2,2001)

# plotting
plt.plot(x,fx(x), x,deriv(x))
plt.xlim(x[[0,-1]])
plt.grid()
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend(['y','dy'])
plt.show()
```

```python
# random starting point
localmin = np.random.choice(x,1)
initval = localmin[:]

# learning parameters
learning_rate = .01
training_epochs = 50

# run through training and store all results
modelparamsFixed = np.zeros((training_epochs,3))
for i in range(training_epochs):
  grad = deriv(localmin)

  lr = learning_rate
    
  localmin = localmin - lr*grad

  modelparamsFixed[i,:] = localmin[0],grad[0],lr
```


```python
# random starting point
localmin = np.random.choice(x,1)
initval = localmin[:]

# learning parameters
learning_rate = .01
training_epochs = 50

# run through training and store all results
modelparamsGrad = np.zeros((training_epochs,3))
for i in range(training_epochs):
  grad = deriv(localmin)
  
  # adapt the learning rate according to the gradient
  lr = learning_rate * np.abs(grad)
    
  localmin = localmin - lr*grad

  modelparamsGrad[i,:] = localmin[0],grad[0],lr[0]
```

```python
# redefine parameters
learning_rate =.1
localmin=initval

# run through training and store all the results
modelparamsTime = np.zeros((training_epochs,3))
for i in range(training_epochs):
    grad = deriv(localmin)
    lr = learning_rate*(1-(i+1)/training_epochs)
    localmin = localmin - lr*grad
    modelparamsTime[i,:] = localmin[0],grad[0],lr
```

```python
# plot the results
fig, ax = plt.subplots(1,3,figsize=(10,3))

# generate the plots
for i in range(3):
    ax[i].plot(modelparamsFixed[:,i],'o-',markerfacecolor='w')
    ax[i].plot(modelparamsGrad[:,i],'o-',markerfacecolor='w')
    ax[i].plot(modelparamsTime[:,i],'o-',markerfacecolor='w')
    ax[i].set_xlabel('Iteration')

ax[0].set_ylabel('Local minimum')
ax[1].set_ylabel('Derivative')
ax[2].set_ylabel('Learning rate')
ax[2].legend(['Fixed l.r','Grad-based l.r','Time-based l.r.'])

plt.tight_layout()
plt.show()
```
![](37.,Pasted%20image%2020241020132021.png)

---
## Summary and thoughts

학습률을 조정하는 가능한 방법들:

1) <span style="color:rgb(116, 195, 194)">훈련 에포크 :</span> <span style="color:rgb(236, 158, 111)">좋은방법이며, 종종 블록 단위로 수행됨.</span> <span style="color:rgb(230, 122, 122)">그러나 모델의 성능이나 정확도와 직접적인 관련이 없음</span> 
		<span style="color:rgb(205, 205, 81)">학습 감쇠(learning rate decay)라고 불림</span>

2) <span style="color:rgb(116, 195, 194)">도함수: </span><span style="color:rgb(236, 158, 111)">문제에 적용됨.</span> <span style="color:rgb(230, 122, 122)">추가적인 파라미터와 적절한 스케일링이 필요함</span> 
   <span style="color:rgb(205, 205, 81)">RMSprop와 Adam 옵티마이저에 통합되어 있음</span>
   
3) <span style="color:rgb(116, 195, 194)">손실:</span> <span style="color:rgb(236, 158, 111)">문제에 적응됨.</span> <span style="color:rgb(230, 122, 122)">손실값이[0,1] 범위 안에 있을때만 동작함(스케일링 가능)</span>
4) <span style="color:rgb(116, 195, 194)">현재 지역 최소값:</span> <span style="color:rgb(236, 158, 111)">문제에 적응적으로 반응함.</span> <span style="color:rgb(230, 122, 122)">하지만 너무 많은 가정이 필요</span> 

	