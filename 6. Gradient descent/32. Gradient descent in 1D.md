- 어떻게 경사 하강법이 코드에서 구현되는지
- learning rate와 training epochs라는 2개의 파라미터가 어떻게 구현되는지 학습
- 왜 경사하강법이 올바른 답을 보장하지 않는지에 대한 이해

---
## The function and its derivative

![32.Pasted image 20241010212635](../pic/6.%20Gradient%20descent/32.Pasted%20image%2020241010212635.png)

	참고 : 상수는 미분 시 사라짐

![32.Pasted image 20241010213313](../pic/6.%20Gradient%20descent/32.Pasted%20image%2020241010213313.png)

	 현재 도함수가 0이 되는곳이 최소값이며, 그 지점은 .5이다.

---
## Code

```python
import numpy as np
import matplotlib.pyplot as plt

from IPython import display
display.set_matplotlib_formats('svg')
```

```python
# function (as a function)
def fx(x):
    return 3*x**2 - 3*x+4

# derivate funtion
def deriv(x):
    return 6*x -3
```

```python
# plot the function and its derivative

# define a range for x
x = np.linspace(-2,2,2001)

# plotting
plt.plot(x,fx(x),x,deriv(x))
plt.xlim(x[[0,-1]])
plt.grid()
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend(['y','dy'])
plt.show()
```
![32.Pasted image 20241014204001](../pic/6.%20Gradient%20descent/32.Pasted%20image%2020241014204001.png)


```python
# random starting point
localmin = np.random.choice(x,1)
print(localmin)

# learning parameters
learning_rate = .01
training_epochs = 100

# run through training
for i in range(training_epochs):
    grad = deriv(localmin)
    localmin = localmin - learning_rate*grad

localmin
```
```
[-1.244]

array([0.4964163])
```

```python
plt.plot(x,fx(x),x,deriv(x))
plt.plot(localmin,deriv(localmin),'ro')
plt.plot(localmin,fx(localmin),'ro')

plt.xlim(x[[0,-1]])
plt.grid()
plt.xlabel('x')
plt.ylabel('f(x)')
plt.legend(['f(x)','df','f(x) min'])
plt.title('Empirical local minimum: %s'%localmin[0])
plt.show()
```
![32.Pasted image 20241014204103](../pic/6.%20Gradient%20descent/32.Pasted%20image%2020241014204103.png)

## Store the model parameters and outputs on each iteration


```python
# random starting point
localmin = np.random.choice(x,1)

# learning parameters
learning_rate = .01
training_epochs = 100

# run through training and store all the results
modelparams = np.zeros((training_epochs,2))
for i in range(training_epochs):
    grad = deriv(localmin)
    localmin = localmin-learning_rate*grad
    modelparams[i,:] =localmin[0],grad[0]
```

```python
# plot the gradient over iterations
fig,ax = plt.subplots(1,2,figsize=(12,4))

for i in range(2):
    ax[i].plot(modelparams[:,i],'o-')
    ax[i].set_xlabel('Iteration')
    ax[i].set_title(f'Final estimated minimum: {localmin[0]:.5f}')

ax[0].set_ylabel('Local minimum')
ax[1].set_ylabel('Derivative')

plt.show()
```
![32.Pasted image 20241014204157](../pic/6.%20Gradient%20descent/32.Pasted%20image%2020241014204157.png)

## Additional explorations

```python
# 1) 딥러닝에서 모델은 보통 정해진 횟수의 반복(iterations) 동안 학습을 진행합니다. 우리가 여기서 하는 것도 그 방식입니다. 
#    하지만 학습이 얼마나 오래 지속되는지 정의하는 다른 방법들도 있습니다. 코드를 수정하여, 파생값(도함수)이 
#    일정한 임계값(e.g., 0.1)보다 작아지면 학습이 종료되도록 만들어 보세요. 코드가 음의 파생값(도함수)에서도 
#    정상적으로 작동하도록 신경쓰세요.

# random starting point
localmin = np.random.choice(x,1)

# learning parameters
learning_rate = .01
#training_epochs = 100

# run through training and store all the results
modelparams = np.zeros((training_epochs,2))
#for i in range(training_epochs):
i = 0
while True:
    grad = deriv(localmin)
    localmin = localmin-learning_rate*grad
    modelparams[i,:] =localmin[0],grad[0]
    if abs(modelparams[:,1][i]) < 0.1: break
    i+=1

# -----
# 2) 이 코드 변경이 더 정확한 결과를 만들어 내나요? 정지 임계값을 변경하면 어떻게 될까요?
# me: 0.1임계값 가지고는 더 정확한 결과를 만들어 낼 수 없음,임계값을 낮추면 낮출수록 배열의 크기 동적으로 변경해야함

# 3) 학습이 정해진 에포크(epoch) 수 대신, 파생값(도함수)을 기준으로 종료되도록 했을 때 발생할 수 있는 
#    잠재적인 문제점은 무엇이 있을까요?
# me : 학습률이 매우 작을 경우 무한루프에 가까운 반복이 진행됨./지역 최소값이 존재하는 함수의 경우, 임계값에 도달하지 못하여 무한루프
```