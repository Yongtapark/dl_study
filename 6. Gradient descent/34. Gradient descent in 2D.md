- 2차원 경사하강법과 1차원 경사하강법이 동일하다는것을 확인
- 전역 최소값을 찾는 것의 어려움을 시각적으로 이해

---
## Quick note about terminology

![34.Pasted image 20241016205616](34.Pasted%20image%2020241016205616.png)

	경사(gradient) : 해당 함수의 모든 차원에 대한 편미분의 집합 -> 도함수와 본질적으로 동일, 차원이 늘어날뿐
	편미분(partial derivative) : 한 차원을 무시하고 다른 차원에만 집중한 함수의 미분 

---
## Quick note about notations

<span style="color:rgb(118, 147, 234)">도함수(Derivative)</span> 
![34.Pasted image 20241016210024](34.Pasted%20image%2020241016210024.png)

<span style="color:rgb(118, 147, 234)">편도함수</span> <span style="color:rgb(118, 147, 234)">(Partial derivative)</span>
![Pasted image 20241016210144](34.Pasted%20image%2020241016210144.png)

<span style="color:rgb(118, 147, 234)">경사(Gradient)</span>  $\nabla$ : nabla = 데이터의 모든 방향 또는 차원에 대한 도함수의 집합
![Pasted image 20241016210158](34.Pasted%20image%2020241016210158.png)

---
## Repeat in 2D

![Pasted image 20241016210734](34.Pasted%20image%2020241016210734.png)	

	2개의 변수(x,y)를 가진 함수 -> 2차원 함수

	위 함수를 x와 y위치에서 그래프로 그린다면 아래와 같음.
	노란색은 피크(봉우리)에 해당하고, 파란색은 계곡에 해당함
![Pasted image 20241016210915](34.Pasted%20image%2020241016210915.png)

<span style="color:rgb(205, 205, 81)">sympy와 lambdify를 사용하여 함수의 편도함수를 계산하라</span>

<span style="color:rgb(236, 158, 111)">경사하강 반복 루프를 수행하라</span>

<span style="color:rgb(118, 147, 234)">지역 최소값이 (x,y)임을 확인하라</span>

<span style="color:rgb(116, 195, 194)">시각화하라!</span>

![Pasted image 20241016211546](34.Pasted%20image%2020241016211546.png)

---
## Code

```python
import numpy as np
import matplotlib.pyplot as plt
import sympy as sym # sympy to compute the partial derivatives

from IPython import display
display.set_matplotlib_formats('svg')
```

## Gradient descent in 2D
```python
# the "peaks" function
def peaks(x,y):
    # expand to a 2D mesh
    x,y = np.meshgrid(x,y)

    z = 3*(1-x)**2 * np.exp(-(x**2)-(y+1)**2)\
    -10*(x/5-x**3-y**5)*np.exp(-x**2-y**2)\
    -1/3*np.exp(-(x+1)**2-y**2)
    return z
```

```python
# create the landscape
x = np.linspace(-3,3,201)
y = np.linspace(-3,3,201)

Z = peaks(x,y)

# let's have a look!
plt.imshow(Z,extent=[x[0],x[-1],y[0],y[-1]],vmin=-5,vmax=5,origin='lower')
plt.show()
```
![Pasted image 20241016221713](34.Pasted%20image%2020241016221713.png)

```python
# create derivative functions using sympy

sx,sy = sym.symbols('sx,sy')

sZ = 3*(1-sx)**2 * sym.exp(-(sx**2)-(sy+1)**2)\
-10*(sx/5-sx**3-sy**5)*sym.exp(-sx**2-sy**2)\
-1/3*sym.exp(-(sx+1)**2-sy**2)

# create functions from the sympy-computed derivatives
#sym.lambdify: Symbolic Object를 numpy 함수로 변환해서 호출할 수 있게 해줌
df_x = sym.lambdify((sx,sy),sym.diff(sZ,sx),'sympy')
df_y = sym.lambdify((sx,sy),sym.diff(sZ,sy),'sympy')

#함수의 (1,1) 지점에서 X에 대한 편미분을 계싼하는 예제
df_x(1,1).evalf()
```

```python
# random starting point (uniform between -2 and _2)
#localmin = np.random.rand(2)*4-2 # also try specifying coordinates
localmin = np.array((0,1.4))
startpnt = localmin[:] # make a copy, not re-assign

# learning parameters
learning_rate = .01
training_epochs =1000

# run through training
trajectory = np.zeros((training_epochs,2))
for i in range(training_epochs):
    grad = np.array([
        df_x(localmin[0],localmin[1]).evalf(),
        df_y(localmin[0],localmin[1]).evalf()
    ])
    localmin = localmin - learning_rate * grad # add _ or [:] to change a variable in-place
    trajectory[i,:] = localmin

print(localmin)
print(startpnt)
```
```
[0.296445553846832 0.320196247666835]
[0.  1.4]
```

```python
# let's have a look!
plt.imshow(Z,extent=[x[0],x[-1],y[0],y[-1]],vmin=-5,vmax=5,origin='lower')
plt.plot(startpnt[0],startpnt[1],'bs')
plt.plot(localmin[0],localmin[1],'ro')
plt.plot(trajectory[:,0],trajectory[:,1],'r')
plt.legend(['rnd start','local min'])
plt.colorbar()
plt.show()
```
![Pasted image 20241016221739](34.Pasted%20image%2020241016221739.png)

## Additional explorations
```python
# 1) 코드를 수정하여 초기 추정값을 [0,1.4]로 강제 설정하세요. 모델이 합리적인 로컬 최소점에 도달하나요?  
# me : 가장 나쁜 지역 최소값에 갇힘

# 2) 동일한 시작 지점을 사용해 학습 에포크 수를 10,000으로 변경하세요. 최종 솔루션이 1,000 에포크를 사용했을 때와 다르게 나옵니까?  
# me : 동일함. 에포크를 늘린다 해서 지역 최소값을 빠져나갈수 없음

# 3) (다시 같은 시작 지점에서) 학습률을 0.1로 설정하고 1,000 에포크로 실행하세요. 경로에서 무엇을 발견했나요?  
#    학습률을 0.5로 설정하고, 그다음 0.00001로 설정하여 다시 시도해 보세요. 
# me : .1로 설정하니 경로가 급격히 꺾이는 부분이 생김, 하지만 여전히 동일한 지역 최소값에 빠짐
#      .5로 설정하니 경로가 이상한 곳으로 튀어나감 (overshooting)
#      .00001로 설정하니, 기존 자리에서 얼마 움직이지 않았음
```