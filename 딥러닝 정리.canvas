{
	"nodes":[
		{"id":"b281c641c1d5c164","type":"group","x":-460,"y":400,"width":9340,"height":1480,"color":"6","label":"5. Math, numpy, PyTorch"},
		{"id":"2cee37c840b0b804","type":"group","x":-460,"y":1880,"width":3580,"height":2040,"color":"6","label":"6. Gradient descent"},
		{"id":"1b131fbb560d69fe","type":"group","x":-460,"y":-153,"width":1760,"height":513,"color":"6","label":"3. Concpets in deep learning "},
		{"id":"78b18ced0cb6a380","type":"text","text":"# 5. What is an artificial neural network?\n\n딥러닝 모델은 비선형 문제에 아주 강함.\n\n![](5.Pasted%20image%2020240928194835.png)\n\n각 유닛들은 간단한 방정식을 가지고 있으며, 이들을 합쳐 복잡한 연산을 생성\n\n용도에 따라 다양한 모델들이 존재하지만, 기본적으로 유사성이 많다.","x":-440,"y":-140,"width":580,"height":480},
		{"id":"5173f9be140cc74e","type":"text","text":"## 6. How models \"learn\" \n\n![](6.Pasted%20image%2020240928212509.png)\n\n<span style=\"color:rgb(236, 158, 111)\">순전파: 돈/서비스가 네트워크를 통해 흐른다</span>\n<span style=\"color:rgb(230, 122, 122)\">역전파 : '오류 메시지'가 사장에게서 다시 부서들을 거쳐 내려온다.</span> \n\n\t중요 개념 : 오류 메시지가 최상위에 있는 오너에게 전달된다는 것. 오너는 마케팅 팀에게 \"상황을 개선하라\"고 지시하고, 이들은 주방 팀에게 \"개선을 위해 무언가를 바꾸라\"고 말하는 식으로 전파됨","x":160,"y":-140,"width":560,"height":480},
		{"id":"d7a371df5cad9367","type":"text","text":"## 8. Running experiments to understand DL\n## Parametric experiments\n\n<span style=\"color:rgb(118, 147, 234)\">모수 실험(Parametric experiment): 하나 또는 두개의 변수를 체계적으로 조작하면서 실험을 반복하는 것.</span>\n\n<span style=\"color:rgb(116, 195, 194)\">독립 변수(Independent variable): 당신이 조작하는 변수들(학습률, 배치 크기, 옵티마이저, 손실 함수,...)</span>\n\n<span style=\"color:rgb(205, 205, 81)\">종속 변수(Dependent variable): 모델 성능을 평가하는 데 사용하는 핵심 결과 변수(정확도, 속도)</span>\n","x":740,"y":-140,"width":540,"height":480},
		{"id":"b80cdeaf7a3dd81f","type":"text","text":"# 13. Spectral theories in mathematics\n\n## Spectral theories and deep learning\n\n![13.Pasted image 20240930214108](../pic/5.%20Math,%20numpy,%20PyTorch/13.Pasted%20image%2020240930214108.png)\n\n\t각각의 노드는 사실 매우 단순한 수학적 연산을 수행함.\n여기($x^Tw$)서 내적(dot product)이라는 연산을 사용함 이후 그 내적을 어떤\n비선형 함수($\\sigma$)에 통과시킴\n","x":-440,"y":420,"width":580,"height":840},
		{"id":"7c840915ec2c2393","type":"text","text":"# 14. Terms and datatypes in math and computers\n## Linear algebra terminology\n\n![14.Pasted image 20240930224237](../pic/5.%20Math,%20numpy,%20PyTorch/14.Pasted%20image%2020240930224237.png)\n\n\t스칼라 : 개별적인 숫자가 하나만 존재하는것\n\t벡터 : 숫자의 열 또는 행\n\t행렬 : 숫자의 2차원 스프레드시트\n\t텐서 : 여기서는 3차원 텐서를 묘사, 숫자의 큐브. 데이터 분석 및 신호처리에서는 항상 텐서를 다룸","x":160,"y":420,"width":580,"height":840},
		{"id":"bc1407b8cdc671f9","type":"text","text":"# 15. Converting reality to numbers\n## Dummy-coding\n\n\t시험 통과 여부\n\n![](15.Pasted%20image%2020241001210755.png)\n\n---\n## One-hot encoding\n\n\t영화 장르\n\n![](15.Pasted%20image%2020241001211301.png)","x":760,"y":420,"width":560,"height":840},
		{"id":"0a03df347e53ef84","type":"text","text":"# 16. Vector and matrix transpose\n\n## Transposing a vector\n\n![](16.Pasted%20image%2020241001212940.png)\n\n\t전치연산은 종종 윗첨자에 대문자 T를 사용하여 표시함\n\t전치 : 행 -> 열, 열 -> 행 으로 변환하는것을 의미\n\n---\n## Transposing a matrix\n\n![](16.Pasted%20image%2020241001213224.png)\n","x":1340,"y":420,"width":600,"height":840},
		{"id":"e132bfb23330244f","type":"text","text":"# 17. OMG it's the dot product!\n\n## Notations for and definition of the dot product\n\n![](17.Pasted%20image%2020241001215506.png)\n\n내적 : a와 b의 n개의 요소에 대해 각각의 대응 요소를 곱하고, 그 결과를 모두 합하는 것\n\n---\n## Dot product (a.k.a. scalar product) example\n\n![](17.Pasted%20image%2020241001215721.png)\n\n\t두 벡터 사이의 내적은 항상 하나의 숫자, 단일값이다.\n\t내적은 두 벡터가 같은 차원을 가질 때만 정의된다.\n\n---\n## Dot product in 2D\n\n\t합성곱(convolution), 합성곱 신경망(CNN)에 사용\n\n![](17.Pasted%20image%2020241001220241.png)\n\n---\n## Interpretation of the dot product\n\n<span style=\"color:rgb(116, 195, 194)\">두 객체(벡터, 행렬, 텐서, 신호, 이미지)간의 공통점을 반영하는 하나의 숫자</span>","x":1960,"y":420,"width":720,"height":1440},
		{"id":"071982ba0b92fc23","type":"text","text":"# 18. Matrix multiplication\n\n## Standard matrix multiplication: The rules for validity\n\n![](18.Pasted%20image%2020241002202434.png)\n\n행렬 곱셈의 유효성 :\n- 곱할때에는 각 행렬의 내부차원(Inner dimension)을 봐야함\n\t여기서는 N 들의 크기가 동일한지를 봐야함\n- 또한 나머지 M과 K는 외부차원이 되며, 곱한 결과의 크기가 된다\n---\n## Matrix multiplication: Examples\n\n![](18.Pasted%20image%2020241002203318.png)\n\nA B : 행렬곱 가능\n\nB A : 내부 차원이 다르므로 불가능\n\n$\\text{C}^{\\text{T}}$ A : 전치하여 행렬곱이 가능\n\n---\n## Matrix multiplication as ordered dot products\n\n![](18.Pasted%20image%2020241002204325.png)\n","x":2680,"y":420,"width":640,"height":1260},
		{"id":"20d8ab030e9b0e6e","type":"text","text":"# 19. Softmax\n\n## The natural exponent\n-자연지수\n\n![](19.Pasted%20image%2020241002211620.png)\n$e^x$의 특징:\n1. 매우 빠르게 무한대로 증가\n2. 절대로 0보다 작아지지 않음\n\n이 특징이 중요한 이유 : 자연 지수를 소프트맥스 함수에 사용하여 확률을 생성하기 위함\n\t확률은 양수 또는 최소한 음수가 아닌 값이어야 함\n\n---\n---\n## The softmax function: numerical example\n\n![](19.Pasted%20image%2020241002212930.png)\n\n시그마($\\sigma$)의 총합은 1이 됨 -> 확률로써 사용\n\n\t이렇게 숫자 집합의 모든 소프트맥스 변환을 합치면 그 결과는 항상 1이 되며, 항상 양수임\n","x":3320,"y":420,"width":680,"height":1260},
		{"id":"759ddc3d1556e8ac","type":"text","text":"# 20. Logarithms\n\n## Logarithm\n![](20.Pasted%20image%2020241003204050.png)\n\n\t자연로그는 자연지수와 마찬가지로 무한대로 증가하지만, x가 커질수록 무한대로 가는 속도가 훨씬 느려짐\n\n<span style=\"color:rgb(116, 195, 194)\">로그는 단조 함수(monotonic function) 이다</span> .\n\n그럼 이것이 왜 중요한가?\n\n<span style=\"color:rgb(236, 158, 111)\">이것은 중요하다. 왜냐하면</span>\n<span style=\"color:rgb(205, 205, 81)\">x를 최소화 하는것이 log(x)를 최소화하는 것과 동일하기 때문이다!</span> \n<span style=\"color:rgb(230, 122, 122)\">(단, x>0 인 경우만 해당)</span> \n\n\t작은 값을 로그 함수로 확장하면 컴퓨터가 계산하기 쉽고 더 안정적으로 처리할 수 있다. \n\t즉, 최적화 문제에서 로그를 통해 매우 작은 값을 다룰 때도 원래의 값이 가진 의미를 잃지 않고 최적화할 수 있게 된다.\n\n<span style=\"color:rgb(116, 195, 194)\">로그는 x의 작은 값들을 확장시킨다.</span> \n\n\t로그의 핵심은 작은 x값들을 확장시킨다는 점이다.\n\n<span style=\"color:rgb(236, 158, 111)\">이것이 중요한 이유는</span> \n<span style=\"color:rgb(205, 205, 81)\">로그가 서로 인접한 작은 수들을 더 잘 구분한다는것을 의미하기 때문이다.</span>\n\n\t컴퓨터는 너무 작은수를 계싼하기 어렵기 때문\n\n\n---\n## Logarithm and machine learning\n\n<span style=\"color:rgb(236, 158, 111)\">머신러닝과 딥러닝에서는 종종 확률과 같은매우 작은 양을 최소화해야 한다. </span>\n\n<span style=\"color:rgb(116, 195, 194)\">컴퓨터는 이러한 매우 작은 숫자를 다룰 때 정밀도 오류가 발생할 수 있다.</span> \n\n<span style=\"color:rgb(118, 147, 234)\">Take-home: 로그는 ML을 더 잘 동작하게 한다. </span>","x":4000,"y":420,"width":680,"height":1380},
		{"id":"fa92a92610f0dba6","type":"text","text":"# 21. Entropy and cross-entropy\n\n\n---\n## Cross-entropy\n\n![](21.Pasted%20image%2020241004195436.png)\n\n<span style=\"color:rgb(236, 158, 111)\">엔트로피는 하나의 확률 분포를 설명</span> \n\n![](21.Pasted%20image%2020241004195546.png)\n\n<span style=\"color:rgb(205, 205, 81)\">교차 엔트로피는 두 확률 분포 사이의 관계를 설명</span> \n\n<span style=\"color:rgb(205, 205, 81)\">(Note: 딥러닝에서는 사건이 발생 확률, 발생하지 않을 확률 ->  p=0 or p=1)</span> \n\n","x":4680,"y":420,"width":620,"height":580},
		{"id":"f27bc6ef209c4071","type":"text","text":"# 22. Min, max and argmin, argmax\n\n## Min/max, argmin/argmax\n\n![](22.Pasted%20image%2020241004225527.png)\n\n\t수학적 표현에서 arg 에서는 0이 아닌 1부터 시작\n\t하지만 파이썬에서 arg는 0부터 시작","x":5300,"y":420,"width":620,"height":580},
		{"id":"fbbb4237c8a63c0d","type":"text","text":"# 23. Mean and variance\n\n## Mean(a.k.a. arithmetic mean a.k.a. average)\n\n![23.Pasted image 20241005210141](app://c9545181423dd3a5c7157f07c1318de95387/Users/yongtapark/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/ml/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%95%EC%9D%98/pic/5.%20Math,%20numpy,%20PyTorch/23.Pasted%20image%2020241005210141.png?1728129701729)\n\n---\n## Variance\n\n![23.Pasted image 20241005211303](../pic/5.%20Math,%20numpy,%20PyTorch/23.Pasted%20image%2020241005211303.png)\n\n---\n## Standard deviation\n\n![23.Pasted image 20241005213206](../pic/5.%20Math,%20numpy,%20PyTorch/23.Pasted%20image%2020241005213206.png)\n","x":5920,"y":420,"width":560,"height":900},
		{"id":"aea60ce94aeb81d9","type":"text","text":"# 24. Sampling variability\n\n## What to do about sampling variability?\n\n<span style=\"color:rgb(205, 205, 81)\">많은 표본을 추출하라!</span> <span style=\"color:rgb(236, 158, 111)\">여러 표본을 평균내면 실제 모집단 평균에 가까워짐 (대수의 법칙)</span>\n\n---\n## Why sampling variability is important in DL\n\n<span style=\"color:rgb(118, 147, 234)\">딥러닝 모델은 예시를 통해학습</span>\n\n<span style=\"color:rgb(116, 195, 194)\">비무작위 표본 추출은 딥러닝 모델에 체계적인 편향을 도입할 수 있음</span>\n\n<span style=\"color:rgb(205, 205, 81)\">대표적이지 않은 표본 추출은 과적합을 일으키고 일반화 가능성을 제한한다</span> ","x":6480,"y":420,"width":520,"height":450},
		{"id":"7a386bb7d164819b","type":"text","text":"# 27. Derivatives- Intuition and polynomials\n\n## Derivative examples\n\n\t도함수 : 각 포인트에서의 기울기 값\n\t변화하는 선의 기울기\n\n![27.Pasted image 20241007200313](../pic/5.%20Math,%20numpy,%20PyTorch/27.Pasted%20image%2020241007200313.png)\n\n---\n## Derivative of a polynomial\n\n![27.Pasted image 20241007202555](../pic/5.%20Math,%20numpy,%20PyTorch/27.Pasted%20image%2020241007202555.png)\n\n","x":7000,"y":420,"width":640,"height":900},
		{"id":"a73cfc01338c35ca","type":"text","text":"# 28. Derivatives find minima\n\n## Finding minima with derivatives\n\n![](28.Pasted%20image%2020241008220942.png)\n\n\t왼쪽에서 오른쪽으로 이동하는 흐르는것을 기준\n\n<span style=\"color:rgb(236, 158, 111)\">최소값:</span>\n<span style=\"color:rgb(236, 158, 111)\">왼쪽으로 갈수록</span> df<0<span style=\"color:rgb(236, 158, 111)\">, 오른쪽으로 갈 수록 </span>df>0\n\n<span style=\"color:rgb(205, 205, 81)\">최대값:</span>\n<span style=\"color:rgb(205, 205, 81)\">왼쪽으로 갈수록 </span>df>0<span style=\"color:rgb(205, 205, 81)\">, 오른쪽으로 갈 수록 </span>df<0\n\n<span style=\"color:rgb(230, 122, 122)\">상수:</span>\n<span style=\"color:rgb(230, 122, 122)\">\"기울기 소실\"</span> \n\n\t도함수가 0이지만 최대값, 최소값이 아닌 제 3의 경우가 존재.(붉은 원) \n\t함수가 일정한 구간. \n\t이러한 상황을 기울기 소실(vanishing gradient)라고 부름\n","x":7640,"y":420,"width":600,"height":900},
		{"id":"1257921d071d973f","type":"text","text":"# 29. Derivatives- product and chain rules\n\n## The product rule of derivatives\n\n![](29.Pasted%20image%2020241009201812.png)\n\n\t두 함수를 더한 후 도함수를 구하는것은 각자의 도함수를 구하여 합한것과 동일함\n\n![](29.Pasted%20image%2020241009201843.png)\n\n\t하지만 곱하여 도함수를 구한것과 각자 도함수를 구하여 곱한것은 다르다.\n\n![](29.Pasted%20image%2020241009201941.png)\n\n\t 두 함수를 곱한 도함수는 (첫번째 도함수 * 두번쨰 함수) + (첫번재 함수 * 두번쨰 도함수)와 같다.\n ---\n## The chain rule of derivatives\n\n![](29.Pasted%20image%2020241009202851.png)\n","x":8240,"y":420,"width":600,"height":980},
		{"id":"ddee131bf8b4d10b","type":"text","text":"# 30. Overview of gradient descent\n\n## Gradient descent algorithm\n\n<span style=\"color:rgb(205, 205, 81)\">최소값을 임의로 추정하여 초기화</span>\n\n<span style=\"color:rgb(236, 158, 111)\">반복훈련 과정을 반복</span>\n\n-  <span style=\"color:rgb(118, 147, 234)\">추정된 최소값에서 도함수를 계산</span>\n\n-  <span style=\"color:rgb(116, 195, 194)\">업데이트된 추정 최소값은 도함수에서 학습률을 곱한 값을 뺀 값으로 갱신</span>\n\n![](30.Pasted%20image%2020241009214104.png)\n\n\t경험적 최소값이 정확한 답은 아님\n\t이를 증명하기 위해 함수의 진정한 최소값을 해석적으로 계산할 수 있음\n---\n## Gradient descent is wrong???!?\n\n![](30.Pasted%20image%2020241009214319.png)\n\n\t경사 하강법은 실제로 거의 맞는 답을 제공함 하지만 위 함수에 대한 정확한 해가 아님.\n\n\t여러가지 상황에서 경사 하강법알고리즘이 부정확한 경우가 있음\n\t\n\t1. 모델 매개변수를 잘못 선택한 경우\n\t2. 지역 최소값에 갇히는 경우\n\t3. 소실된 경사 혹은 폭발적인 경사와 같은 문제\n","x":-440,"y":1900,"width":690,"height":1420},
		{"id":"2c1ed56a9f2a06c5","type":"text","text":"# 31. What about local minima\n\n## Local vs. global minima\n\n\t1. 많은 해결책이 존재할 가능성이 있다.\n\n![](31.Pasted%20image%2020241010204622.png)\n\n---\n## Or maybe there aren't so many local minima!\n\n\t..또는 고차원 공간에서는 실제로 그렇게 많은 국소 최소값이 존재하지 않을 수 있다.\n\n![](31.Pasted%20image%2020241010205515.png)\n\n\t1차원과는 달리 고차원 고차원 공간에서는 지역 최소값이 되려면 모든 방향에서 최소값이 되어야 함.\n\n<span style=\"color:rgb(236, 158, 111)\">지역 최소값.</span>\n<span style=\"color:rgb(236, 158, 111)\">단일 방향</span>\n\n<span style=\"color:rgb(116, 195, 194)\">지역 최대값</span>\n<span style=\"color:rgb(116, 195, 194)\">다른 방향</span>\n\n\t안정점(saddle point) : 한 방향에서 최소값을 가지고, 다른 방햐에서 최대값을 갖는 지점\n\n<span style=\"color:rgb(230, 122, 122)\">경사하강법은 그 지점이 모든 차원에서 최소값일 때에만 지역 최소값에 갇히게 됨</span> \n\n\t만약 40만개의 파라미터를 가지고 있다면, 국소 최소값은 모든 40만 차원에서 최소값이어야 하며, 그 지점의 도함수는 40만개의 모든 차원에서 0이어야 한다.\n\n<span style=\"color:rgb(116, 195, 194)\">극도로 고차원 공간에서는, 지역 최소값이 매우 적을 수 있다.</span> \n\n\t딥러닝의 맥락에서 매우 풍부한 고차원 공간을 다룰 때, 지역 최소값이 경사 하강법 알고리즘에서 그리 큰 문제가 아닐수 있음.\n\t하지만 지역 최소값에 갇혀 있는지 여부를 정확히 알기는 어렵다.\n\t\n\t그렇다면 이를 어떻게 해결할 수있을까?\n\n---\n## What to do about it?\n\n<span style=\"color:rgb(118, 147, 234)\">모델 성능이 좋을 때에는 걱정할 필요가 없음</span> \n\n<span style=\"color:rgb(116, 195, 194)\">가능한 해결법 한가지:</span> <span style=\"color:rgb(205, 205, 81)\">다른 무작위 가중치를 가지고 모델을 여러번 훈련시킨다.(loss landscape의 서로 다른 위치에서 시작) 그 후 최상의 모델을 선택한다.</span> \n\n<span style=\"color:rgb(116, 195, 194)\">다른 가능한 해결법:</span>  <span style=\"color:rgb(205, 205, 81)\">모델의 지역 최소값을 줄이기 위해 차원(복잡성)을 증가시킨다.</span> ","x":250,"y":1900,"width":790,"height":1980},
		{"id":"81b7bda32d8522df","type":"text","text":"# 32. Gradient descent in 1D\n\n## Code\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom IPython import display\ndisplay.set_matplotlib_formats('svg')\n```\n\n```python\n# function (as a function)\ndef fx(x):\n    return 3*x**2 - 3*x+4\n\n# derivate funtion\ndef deriv(x):\n    return 6*x -3\n```\n\n```python\n# plot the function and its derivative\n\n# define a range for x\nx = np.linspace(-2,2,2001)\n\n# plotting\nplt.plot(x,fx(x),x,deriv(x))\nplt.xlim(x[[0,-1]])\nplt.grid()\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.legend(['y','dy'])\nplt.show()\n```\n![](32.Pasted%20image%2020241014204001.png)\n\n\n```python\n# random starting point\nlocalmin = np.random.choice(x,1)\nprint(localmin)\n\n# learning parameters\nlearning_rate = .01\ntraining_epochs = 100\n\n# run through training\nfor i in range(training_epochs):\n    grad = deriv(localmin)\n    localmin = localmin - learning_rate*grad\n\nlocalmin\n```\n```\n[-1.244]\n\narray([0.4964163])\n```\n\n```python\nplt.plot(x,fx(x),x,deriv(x))\nplt.plot(localmin,deriv(localmin),'ro')\nplt.plot(localmin,fx(localmin),'ro')\n\nplt.xlim(x[[0,-1]])\nplt.grid()\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.legend(['f(x)','df','f(x) min'])\nplt.title('Empirical local minimum: %s'%localmin[0])\nplt.show()\n```\n![](32.Pasted%20image%2020241014204103.png)\n\n## Store the model parameters and outputs on each iteration\n\n\n```python\n# random starting point\nlocalmin = np.random.choice(x,1)\n\n# learning parameters\nlearning_rate = .01\ntraining_epochs = 100\n\n# run through training and store all the results\nmodelparams = np.zeros((training_epochs,2))\nfor i in range(training_epochs):\n    grad = deriv(localmin)\n    localmin = localmin-learning_rate*grad\n    modelparams[i,:] =localmin[0],grad[0]\n```\n\n```python\n# plot the gradient over iterations\nfig,ax = plt.subplots(1,2,figsize=(12,4))\n\nfor i in range(2):\n    ax[i].plot(modelparams[:,i],'o-')\n    ax[i].set_xlabel('Iteration')\n    ax[i].set_title(f'Final estimated minimum: {localmin[0]:.5f}')\n\nax[0].set_ylabel('Local minimum')\nax[1].set_ylabel('Derivative')\n\nplt.show()\n```\n![](32.Pasted%20image%2020241014204157.png)\n\n## Additional explorations\n\n```python\n# 1) 딥러닝에서 모델은 보통 정해진 횟수의 반복(iterations) 동안 학습을 진행합니다. 우리가 여기서 하는 것도 그 방식입니다. \n#    하지만 학습이 얼마나 오래 지속되는지 정의하는 다른 방법들도 있습니다. 코드를 수정하여, 파생값(도함수)이 \n#    일정한 임계값(e.g., 0.1)보다 작아지면 학습이 종료되도록 만들어 보세요. 코드가 음의 파생값(도함수)에서도 \n#    정상적으로 작동하도록 신경쓰세요.\n\n# random starting point\nlocalmin = np.random.choice(x,1)\n\n# learning parameters\nlearning_rate = .01\n#training_epochs = 100\n\n# run through training and store all the results\nmodelparams = np.zeros((training_epochs,2))\n#for i in range(training_epochs):\ni = 0\nwhile True:\n    grad = deriv(localmin)\n    localmin = localmin-learning_rate*grad\n    modelparams[i,:] =localmin[0],grad[0]\n    if abs(modelparams[:,1][i]) < 0.1: break\n    i+=1\n\n# -----\n# 2) 이 코드 변경이 더 정확한 결과를 만들어 내나요? 정지 임계값을 변경하면 어떻게 될까요?\n# me: 0.1임계값 가지고는 더 정확한 결과를 만들어 낼 수 없음,임계값을 낮추면 낮출수록 배열의 크기 동적으로 변경해야함\n\n# 3) 학습이 정해진 에포크(epoch) 수 대신, 파생값(도함수)을 기준으로 종료되도록 했을 때 발생할 수 있는 \n#    잠재적인 문제점은 무엇이 있을까요?\n# me : 학습률이 매우 작을 경우 무한루프에 가까운 반복이 진행됨./지역 최소값이 존재하는 함수의 경우, 임계값에 도달하지 못하여 무한루프\n```","x":1040,"y":1900,"width":680,"height":660},
		{"id":"bb08b6e36ae37ea3","x":1720,"y":1900,"width":520,"height":660,"type":"text","text":"# 33. CodeChallenge - unfortunate starting value\n\n## Goals for this code challenge\n\n<span style=\"color:rgb(118, 147, 234)\">Step 1: 1차원 경사하강법을 반복하여 다음 함수의 최소값을 찾아라:</span>\n\n![33.Pasted image 20241014205550](../pic/6.%20Gradient%20descent/33.Pasted%20image%2020241014205550.png)\n\n<span style=\"color:rgb(118, 147, 234)\">Step 2: 시작값으로 x=0을 하드코딩하라</span> \n\n---\n## Code\n\n```python\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom IPython import display\ndisplay.set_matplotlib_formats('svg')\n```\n\n```python\n# function (as a function)\ndef fx(x):\n    return np.cos(2*np.pi*x)+(x**2)\n\n# derivate funtion\ndef deriv(x):\n    return -(2*np.pi)*np.sin(2*np.pi*x)+(2*x)\n\n```\n\n```python\n# plot the function and its derivative\n\n# define a range for x\nx = np.linspace(-2,2,2001)\n\n# plotting\nplt.plot(x,fx(x),x,deriv(x))\nplt.xlim(x[[0,-1]])\nplt.grid()\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.legend(['y','dy'])\nplt.show()\n```\n![](33.Pasted%20image%2020241015211731.png)\n\n```python\n# random starting point\nlocalmin = np.random.choice(x,1)\nprint(localmin)\n\n# learning parameters\nlearning_rate = .01\ntraining_epochs = 100\n\n# run through training\nfor i in range(training_epochs):\n    grad = deriv(localmin)\n    localmin = localmin - learning_rate*grad\n\nlocalmin\n```\n\n```python\nplt.plot(x,fx(x),x,deriv(x))\nplt.plot(localmin,deriv(localmin),'ro')\nplt.plot(localmin,fx(localmin),'ro')\n\nplt.xlim(x[[0,-1]])\nplt.grid()\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.legend(['f(x)','df','f(x) min'])\nplt.title('Empirical local minimum: %s'%localmin[0])\nplt.show()\n```\n![](33.Pasted%20image%2020241015211749.png)\n```python\n# random starting point\nlocalmin = np.array([0])\nprint(localmin)\n\n# learning parameters\nlearning_rate = .01\ntraining_epochs = 100\n\n# run through training\nfor i in range(training_epochs):\n    grad = deriv(localmin)\n    localmin = localmin - learning_rate*grad\n\n\nplt.plot(x,fx(x),x,deriv(x))\nplt.plot(localmin,deriv(localmin),'ro')\nplt.plot(localmin,fx(localmin),'ro')\n\nplt.xlim(x[[0,-1]])\nplt.grid()\nplt.xlabel('x')\nplt.ylabel('f(x)')\nplt.legend(['f(x)','df','f(x) min'])\nplt.title('Empirical local minimum: %s'%localmin[0])\nplt.show()\n\n# 기울기 소실 때문. 함수의 변화가 없는 구간이 존재한다.\n# 기울기를 결정하는 공식은 다음과 같은데\n# for i in range(training_epochs):\n#    grad = deriv(localmin)\n#    localmin = localmin - learning_rate*grad\n\n# 함수 공식(-(2*np.pi)*np.sin(2*np.pi*x)+(2*x))에서도 x=0일때 값이 0이므로 기울기가 업데이트 되지 않음\n```\n![](33.Pasted%20image%2020241015211828.png)\n\n```python\n# 1.\t도함수에 2라는 곱셈 상수가 포함되어 있습니다. 그 상수는 경사 하강법(g.d.) 결과의 정확성을 위해 필수적인가요? \n#       그 도함수에서 ‘2’를 제거하고 모델이 여전히 최솟값을 찾을 수 있는지 확인해보세요.\n#       코드를 실행하기 전에, 어떤 결과를 예상하는지 생각해보세요. 현실이 예상과 일치하나요?\n#       그 상수가 필수적인 이유는 무엇인가요, 또는 필수적이지 않은 이유는 무엇인가요?\n\n# me : 모든 2 상수가 제거되면 도함수의 파동이 간격이 넓어지면서 지역최소값에 도달하지 못함. \n#      모든 2 상수를 제거하는 대신 기존 함수 내의 상수(2*np.pi*x)만 변경하지 않는다면 최소값에 어느정도 근사하게 접근함\n\n# 2.\tnp.sin() 함수 안에 있는 ‘2’라는 상수는 중요한가요?\n#       그것을 제거해도 정확한 결과를 얻을 수 있나요?\n\n# me : 매우 중요함. 실험 결과 파동의 간격이 정해지는 부분인것으 보여짐. 제거하게되면 파동간격이 넓어져 제대로된 지역최소값을 얻기 힘듬\n\n# 3.\t초기 값을 작지만 0이 아닌 값으로 설정해보세요, 예를 들어 0.0001 또는 -0.0001로 설정해보세요.\n#       그것이 해결에 도움이 되나요?\n\n# me : 실험결과 0.0000001 까지는 지역최소값에 도달하며, 0.00000001은 지역최소값과 지역최대값의 중간부분,\n#      0.000000001 부터는 기울기소실이 일어난다.\n```"},
		{"id":"99775ed8e4bfe14f","x":2240,"y":1900,"width":640,"height":2020,"type":"text","text":"# 34. Gradient descent in \n\n## Quick note about terminology\n\n![](34.Pasted%20image%2020241016205616.png)\n\n\t경사(gradient) : 해당 함수의 모든 차원에 대한 편미분의 집합 -> 도함수와 본질적으로 동일, 차원이 늘어날뿐\n\t편미분(partial derivative) : 한 차원을 무시하고 다른 차원에만 집중한 함수의 미분 \n\n---\n## Quick note about notations\n\n<span style=\"color:rgb(118, 147, 234)\">도함수(Derivative)</span> \n![](34.Pasted%20image%2020241016210024.png)\n\n<span style=\"color:rgb(118, 147, 234)\">편도함수</span> <span style=\"color:rgb(118, 147, 234)\">(Partial derivative)</span>\n![](34.Pasted%20image%2020241016210144.png)\n\n<span style=\"color:rgb(118, 147, 234)\">경사(Gradient)</span>  $\\nabla$ : nabla = 데이터의 모든 방향 또는 차원에 대한 도함수의 집합\n![](34.Pasted%20image%2020241016210158.png)\n\n---\n## Repeat in 2D\n\n![](34.Pasted%20image%2020241016210734.png)\t\n\n\t2개의 변수(x,y)를 가진 함수 -> 2차원 함수\n\n\t위 함수를 x와 y위치에서 그래프로 그린다면 아래와 같음.\n\t노란색은 피크(봉우리)에 해당하고, 파란색은 계곡에 해당함\n\n<span style=\"color:rgb(205, 205, 81)\">sympy와 lambdify를 사용하여 함수의 편도함수를 계산하라</span>\n\n<span style=\"color:rgb(236, 158, 111)\">경사하강 반복 루프를 수행하라</span>\n\n<span style=\"color:rgb(118, 147, 234)\">지역 최소값이 (x,y)임을 확인하라</span>\n\n<span style=\"color:rgb(116, 195, 194)\">시각화하라!</span>\n\n![](34.Pasted%20image%2020241016211546.png)\n"}
	],
	"edges":[]
}