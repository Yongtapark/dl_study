{
	"nodes":[
		{"id":"b281c641c1d5c164","type":"group","x":-460,"y":400,"width":8980,"height":1480,"color":"6","label":"5. Math, numpy, PyTorch"},
		{"id":"1b131fbb560d69fe","type":"group","x":-460,"y":-153,"width":1760,"height":513,"color":"6","label":"3. Concpets in deep learning "},
		{"id":"78b18ced0cb6a380","type":"text","text":"# 5. What is an artificial neural network?\n\n딥러닝 모델은 비선형 문제에 아주 강함.\n\n![](5.Pasted%20image%2020240928194835.png)\n\n각 유닛들은 간단한 방정식을 가지고 있으며, 이들을 합쳐 복잡한 연산을 생성\n\n용도에 따라 다양한 모델들이 존재하지만, 기본적으로 유사성이 많다.","x":-440,"y":-140,"width":580,"height":480},
		{"id":"5173f9be140cc74e","type":"text","text":"## 6. How models \"learn\" \n\n![](6.Pasted%20image%2020240928212509.png)\n\n<span style=\"color:rgb(236, 158, 111)\">순전파: 돈/서비스가 네트워크를 통해 흐른다</span>\n<span style=\"color:rgb(230, 122, 122)\">역전파 : '오류 메시지'가 사장에게서 다시 부서들을 거쳐 내려온다.</span> \n\n\t중요 개념 : 오류 메시지가 최상위에 있는 오너에게 전달된다는 것. 오너는 마케팅 팀에게 \"상황을 개선하라\"고 지시하고, 이들은 주방 팀에게 \"개선을 위해 무언가를 바꾸라\"고 말하는 식으로 전파됨","x":160,"y":-140,"width":560,"height":480},
		{"id":"d7a371df5cad9367","type":"text","text":"## 8. Running experiments to understand DL\n## Parametric experiments\n\n<span style=\"color:rgb(118, 147, 234)\">모수 실험(Parametric experiment): 하나 또는 두개의 변수를 체계적으로 조작하면서 실험을 반복하는 것.</span>\n\n<span style=\"color:rgb(116, 195, 194)\">독립 변수(Independent variable): 당신이 조작하는 변수들(학습률, 배치 크기, 옵티마이저, 손실 함수,...)</span>\n\n<span style=\"color:rgb(205, 205, 81)\">종속 변수(Dependent variable): 모델 성능을 평가하는 데 사용하는 핵심 결과 변수(정확도, 속도)</span>\n","x":740,"y":-140,"width":540,"height":480},
		{"id":"b80cdeaf7a3dd81f","type":"text","text":"# 13. Spectral theories in mathematics\n\n## Spectral theories and deep learning\n\n![13.Pasted image 20240930214108](../pic/5.%20Math,%20numpy,%20PyTorch/13.Pasted%20image%2020240930214108.png)\n\n\t각각의 노드는 사실 매우 단순한 수학적 연산을 수행함.\n여기($x^Tw$)서 내적(dot product)이라는 연산을 사용함 이후 그 내적을 어떤\n비선형 함수($\\sigma$)에 통과시킴\n","x":-440,"y":420,"width":580,"height":840},
		{"id":"7c840915ec2c2393","type":"text","text":"# 14. Terms and datatypes in math and computers\n## Linear algebra terminology\n\n![14.Pasted image 20240930224237](../pic/5.%20Math,%20numpy,%20PyTorch/14.Pasted%20image%2020240930224237.png)\n\n\t스칼라 : 개별적인 숫자가 하나만 존재하는것\n\t벡터 : 숫자의 열 또는 행\n\t행렬 : 숫자의 2차원 스프레드시트\n\t텐서 : 여기서는 3차원 텐서를 묘사, 숫자의 큐브. 데이터 분석 및 신호처리에서는 항상 텐서를 다룸","x":160,"y":420,"width":580,"height":840},
		{"id":"bc1407b8cdc671f9","type":"text","text":"# 15. Converting reality to numbers\n## Dummy-coding\n\n\t시험 통과 여부\n\n![](15.Pasted%20image%2020241001210755.png)\n\n---\n## One-hot encoding\n\n\t영화 장르\n\n![](15.Pasted%20image%2020241001211301.png)","x":760,"y":420,"width":560,"height":840},
		{"id":"0a03df347e53ef84","type":"text","text":"# 16. Vector and matrix transpose\n\n## Transposing a vector\n\n![](16.Pasted%20image%2020241001212940.png)\n\n\t전치연산은 종종 윗첨자에 대문자 T를 사용하여 표시함\n\t전치 : 행 -> 열, 열 -> 행 으로 변환하는것을 의미\n\n---\n## Transposing a matrix\n\n![](16.Pasted%20image%2020241001213224.png)\n","x":1340,"y":420,"width":600,"height":840},
		{"id":"e132bfb23330244f","type":"text","text":"# 17. OMG it's the dot product!\n\n## Notations for and definition of the dot product\n\n![](17.Pasted%20image%2020241001215506.png)\n\n내적 : a와 b의 n개의 요소에 대해 각각의 대응 요소를 곱하고, 그 결과를 모두 합하는 것\n\n---\n## Dot product (a.k.a. scalar product) example\n\n![](17.Pasted%20image%2020241001215721.png)\n\n\t두 벡터 사이의 내적은 항상 하나의 숫자, 단일값이다.\n\t내적은 두 벡터가 같은 차원을 가질 때만 정의된다.\n\n---\n## Dot product in 2D\n\n\t합성곱(convolution), 합성곱 신경망(CNN)에 사용\n\n![](17.Pasted%20image%2020241001220241.png)\n\n---\n## Interpretation of the dot product\n\n<span style=\"color:rgb(116, 195, 194)\">두 객체(벡터, 행렬, 텐서, 신호, 이미지)간의 공통점을 반영하는 하나의 숫자</span>","x":1960,"y":420,"width":720,"height":1440},
		{"id":"071982ba0b92fc23","type":"text","text":"# 18. Matrix multiplication\n\n## Standard matrix multiplication: The rules for validity\n\n![](18.Pasted%20image%2020241002202434.png)\n\n행렬 곱셈의 유효성 :\n- 곱할때에는 각 행렬의 내부차원(Inner dimension)을 봐야함\n\t여기서는 N 들의 크기가 동일한지를 봐야함\n- 또한 나머지 M과 K는 외부차원이 되며, 곱한 결과의 크기가 된다\n---\n## Matrix multiplication: Examples\n\n![](18.Pasted%20image%2020241002203318.png)\n\nA B : 행렬곱 가능\n\nB A : 내부 차원이 다르므로 불가능\n\n$\\text{C}^{\\text{T}}$ A : 전치하여 행렬곱이 가능\n\n---\n## Matrix multiplication as ordered dot products\n\n![](18.Pasted%20image%2020241002204325.png)\n","x":2680,"y":420,"width":640,"height":1260},
		{"id":"20d8ab030e9b0e6e","type":"text","text":"# 19. Softmax\n\n## The natural exponent\n-자연지수\n\n![](19.Pasted%20image%2020241002211620.png)\n$e^x$의 특징:\n1. 매우 빠르게 무한대로 증가\n2. 절대로 0보다 작아지지 않음\n\n이 특징이 중요한 이유 : 자연 지수를 소프트맥스 함수에 사용하여 확률을 생성하기 위함\n\t확률은 양수 또는 최소한 음수가 아닌 값이어야 함\n\n---\n---\n## The softmax function: numerical example\n\n![](19.Pasted%20image%2020241002212930.png)\n\n시그마($\\sigma$)의 총합은 1이 됨 -> 확률로써 사용\n\n\t이렇게 숫자 집합의 모든 소프트맥스 변환을 합치면 그 결과는 항상 1이 되며, 항상 양수임\n","x":3320,"y":420,"width":680,"height":1260},
		{"id":"759ddc3d1556e8ac","type":"text","text":"# 20. Logarithms\n\n## Logarithm\n![](20.Pasted%20image%2020241003204050.png)\n\n\t자연로그는 자연지수와 마찬가지로 무한대로 증가하지만, x가 커질수록 무한대로 가는 속도가 훨씬 느려짐\n\n<span style=\"color:rgb(116, 195, 194)\">로그는 단조 함수(monotonic function) 이다</span> .\n\n그럼 이것이 왜 중요한가?\n\n<span style=\"color:rgb(236, 158, 111)\">이것은 중요하다. 왜냐하면</span>\n<span style=\"color:rgb(205, 205, 81)\">x를 최소화 하는것이 log(x)를 최소화하는 것과 동일하기 때문이다!</span> \n<span style=\"color:rgb(230, 122, 122)\">(단, x>0 인 경우만 해당)</span> \n\n\t작은 값을 로그 함수로 확장하면 컴퓨터가 계산하기 쉽고 더 안정적으로 처리할 수 있다. \n\t즉, 최적화 문제에서 로그를 통해 매우 작은 값을 다룰 때도 원래의 값이 가진 의미를 잃지 않고 최적화할 수 있게 된다.\n\n<span style=\"color:rgb(116, 195, 194)\">로그는 x의 작은 값들을 확장시킨다.</span> \n\n\t로그의 핵심은 작은 x값들을 확장시킨다는 점이다.\n\n<span style=\"color:rgb(236, 158, 111)\">이것이 중요한 이유는</span> \n<span style=\"color:rgb(205, 205, 81)\">로그가 서로 인접한 작은 수들을 더 잘 구분한다는것을 의미하기 때문이다.</span>\n\n\t컴퓨터는 너무 작은수를 계싼하기 어렵기 때문\n\n\n---\n## Logarithm and machine learning\n\n<span style=\"color:rgb(236, 158, 111)\">머신러닝과 딥러닝에서는 종종 확률과 같은매우 작은 양을 최소화해야 한다. </span>\n\n<span style=\"color:rgb(116, 195, 194)\">컴퓨터는 이러한 매우 작은 숫자를 다룰 때 정밀도 오류가 발생할 수 있다.</span> \n\n<span style=\"color:rgb(118, 147, 234)\">Take-home: 로그는 ML을 더 잘 동작하게 한다. </span>","x":4000,"y":420,"width":680,"height":1380},
		{"id":"fa92a92610f0dba6","type":"text","text":"# 21. Entropy and cross-entropy\n\n\n---\n## Cross-entropy\n\n![](21.Pasted%20image%2020241004195436.png)\n\n<span style=\"color:rgb(236, 158, 111)\">엔트로피는 하나의 확률 분포를 설명</span> \n\n![](21.Pasted%20image%2020241004195546.png)\n\n<span style=\"color:rgb(205, 205, 81)\">교차 엔트로피는 두 확률 분포 사이의 관계를 설명</span> \n\n<span style=\"color:rgb(205, 205, 81)\">(Note: 딥러닝에서는 사건이 발생 확률, 발생하지 않을 확률 ->  p=0 or p=1)</span> \n\n","x":4680,"y":420,"width":620,"height":580},
		{"id":"f27bc6ef209c4071","type":"text","text":"# 22. Min, max and argmin, argmax\n\n## Min/max, argmin/argmax\n\n![](22.Pasted%20image%2020241004225527.png)\n\n\t수학적 표현에서 arg 에서는 0이 아닌 1부터 시작\n\t하지만 파이썬에서 arg는 0부터 시작","x":5300,"y":420,"width":620,"height":580},
		{"id":"fbbb4237c8a63c0d","type":"text","text":"# 23. Mean and variance\n\n## Mean(a.k.a. arithmetic mean a.k.a. average)\n\n![23.Pasted image 20241005210141](app://c9545181423dd3a5c7157f07c1318de95387/Users/yongtapark/Library/Mobile%20Documents/iCloud~md~obsidian/Documents/ml/%EB%94%A5%EB%9F%AC%EB%8B%9D%20%EA%B0%95%EC%9D%98/pic/5.%20Math,%20numpy,%20PyTorch/23.Pasted%20image%2020241005210141.png?1728129701729)\n\n---\n## Variance\n\n![23.Pasted image 20241005211303](../pic/5.%20Math,%20numpy,%20PyTorch/23.Pasted%20image%2020241005211303.png)\n\n---\n## Standard deviation\n\n![23.Pasted image 20241005213206](../pic/5.%20Math,%20numpy,%20PyTorch/23.Pasted%20image%2020241005213206.png)\n","x":5920,"y":420,"width":560,"height":900},
		{"id":"aea60ce94aeb81d9","type":"text","text":"# 24. Sampling variability\n\n## What to do about sampling variability?\n\n<span style=\"color:rgb(205, 205, 81)\">많은 표본을 추출하라!</span> <span style=\"color:rgb(236, 158, 111)\">여러 표본을 평균내면 실제 모집단 평균에 가까워짐 (대수의 법칙)</span>\n\n---\n## Why sampling variability is important in DL\n\n<span style=\"color:rgb(118, 147, 234)\">딥러닝 모델은 예시를 통해학습</span>\n\n<span style=\"color:rgb(116, 195, 194)\">비무작위 표본 추출은 딥러닝 모델에 체계적인 편향을 도입할 수 있음</span>\n\n<span style=\"color:rgb(205, 205, 81)\">대표적이지 않은 표본 추출은 과적합을 일으키고 일반화 가능성을 제한한다</span> ","x":6480,"y":420,"width":520,"height":450},
		{"id":"7a386bb7d164819b","type":"text","text":"# 27. Derivatives- Intuition and polynomials\n\n## Derivative examples\n\n\t도함수 : 각 포인트에서의 기울기 값\n\t변화하는 선의 기울기\n\n![27.Pasted image 20241007200313](../pic/5.%20Math,%20numpy,%20PyTorch/27.Pasted%20image%2020241007200313.png)\n\n---\n## Derivative of a polynomial\n\n![27.Pasted image 20241007202555](../pic/5.%20Math,%20numpy,%20PyTorch/27.Pasted%20image%2020241007202555.png)\n\n","x":7000,"y":420,"width":640,"height":900},
		{"id":"a73cfc01338c35ca","x":7640,"y":420,"width":600,"height":900,"type":"text","text":"# 28. Derivatives find minima\n\n## Finding minima with derivatives\n\n![](28.Pasted%20image%2020241008220942.png)\n\n\t왼쪽에서 오른쪽으로 이동하는 흐르는것을 기준\n\n<span style=\"color:rgb(236, 158, 111)\">최소값:</span>\n<span style=\"color:rgb(236, 158, 111)\">왼쪽으로 갈수록</span> df<0<span style=\"color:rgb(236, 158, 111)\">, 오른쪽으로 갈 수록 </span>df>0\n\n<span style=\"color:rgb(205, 205, 81)\">최대값:</span>\n<span style=\"color:rgb(205, 205, 81)\">왼쪽으로 갈수록 </span>df>0<span style=\"color:rgb(205, 205, 81)\">, 오른쪽으로 갈 수록 </span>df<0\n\n<span style=\"color:rgb(230, 122, 122)\">상수:</span>\n<span style=\"color:rgb(230, 122, 122)\">\"기울기 소실\"</span> \n\n\t도함수가 0이지만 최대값, 최소값이 아닌 제 3의 경우가 존재.(붉은 원) \n\t함수가 일정한 구간. \n\t이러한 상황을 기울기 소실(vanishing gradient)라고 부름\n"}
	],
	"edges":[]
}