- 엔트로피를 어떻게 해석하는지
- 엔트로피 공식
- 딥러닝에서 엔트로피의 주요 응용
---
## Formula for entropy

![](22.Pasted%20image%2020241004194021.png)

<span style="color:rgb(236, 158, 111)">높은 엔트로피는 데이터셋의 변동성이 많다는것을 의미</span>
<span style="color:rgb(205, 205, 81)">낮은 엔트로피는 데이터셋의 값 대부분이 반복된다는 것(따라서 중복된다는 것)을 의미</span> 

<span style="color:rgb(236, 158, 111)">엔트로피는 분산과 어떻게 다른가?</span> 
<span style="color:rgb(205, 205, 81)">엔트로피는 비선형이며, 분포에 대한 가정을 하지 않음</span> 
<span style="color:rgb(116, 195, 194)">분산은 평균의 유효성에 의존하며 그러므로 대략적 정규 분포에 적절하다.</span> 

![](21.Pasted%20image%2020241004194617.png)

---
## Cross-entropy

![](21.Pasted%20image%2020241004195436.png)

<span style="color:rgb(236, 158, 111)">엔트로피는 하나의 확률 분포를 설명</span> 

![](21.Pasted%20image%2020241004195546.png)

<span style="color:rgb(205, 205, 81)">교차 엔트로피는 두 확률 분포 사이의 관계를 설명</span> 

<span style="color:rgb(205, 205, 81)">(Note: 딥러닝에서는 사건이 발생 확률, 발생하지 않을 확률 ->  p=0 or p=1)</span> 

---
## Code 

## Reminder of entropy:
$$H(p)=-\sum_xp(x)log(p(x))$$

```python
# probability of an event happening
p = .25

# NOT the correct formula!
H = -(p*np.log(p))
print('Wrong entropy: '+str(H))
```
```
Wrong entropy: 0.34657359027997264
```

```python
# the correct way to compute entropy
x = [.25,.75]

H = 0
for p in x:
    H -= p*np.log(p)

print('Correct entropy: '+str(H))
```
```
Correct entropy: 0.5623351446188083
```

```python
# also correct, written out for N=2 events
# Binary entropy

H = -(p*np.log(p)+(1-p)*np.log(1-p))
print('Correct entropy: '+str(H))
```
```
Correct entropy: 0.5623351446188083
```

## Cross-entropy
$$H(p,q)=-\sum{p}\log{(q)}$$

```python
# note : all probs must sum to 1!
p = [1,0] # sum=1
q = [.25,.75] # sum=1

H = 0
for i in range(len(p)):
    H-=p[i]*np.log(q[i])

print('Cross entropy: '+str(H))
```
```
Cross entropy: 1.3862943611198906
```

```python
# also correct, written out for N=2 events
H = -(p[0]*np.log(q[0]) + p[1]*np.log(q[1]))
print('Correct entropy: '+str(H))

# simplification 
# 현 p값이 1과 0이며 p[1]=0이므로, 어차피 0이 나옴. 그렇기에 아래와 같이 단순화 가능
H = -np.log(q[0])
print('Manually simplified: '+str(H))
```
```
Correct entropy: 1.3862943611198906
Manually simplified: 1.3862943611198906
```

```python
# now using pytorch
import torch
import torch.nn.functional as F

# note: inputs must be Tensors
q_tensor = torch.Tensor(q)
p_tensor = torch.Tensor(p)

F.binary_cross_entropy(q_tensor,p_tensor)
```
```
tensor(1.3863)
```