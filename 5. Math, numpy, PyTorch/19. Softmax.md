- 소프트맥스의 수학적 공식
- 소프트맥스의 해석과 목적
---
## The natural exponent
-자연지수

![19.Pasted image 20241002211620](../pic/5.%20Math,%20numpy,%20PyTorch/19.Pasted%20image%2020241002211620.png)
$e^x$의 특징:
1. 매우 빠르게 무한대로 증가
2. 절대로 0보다 작아지지 않음

이 특징이 중요한 이유 : 자연 지수를 소프트맥스 함수에 사용하여 확률을 생성하기 위함
	확률은 양수 또는 최소한 음수가 아닌 값이어야 함

---
## The softmax formula

![19.Pasted image 20241002212551](../pic/5.%20Math,%20numpy,%20PyTorch/19.Pasted%20image%2020241002212551.png)

z : 숫자들의 집합

---
## The softmax function: numerical example

![19.Pasted image 20241002212930](../pic/5.%20Math,%20numpy,%20PyTorch/19.Pasted%20image%2020241002212930.png)

시그마($\sigma$)의 총합은 1이 됨 -> 확률로써 사용

	이렇게 숫자 집합의 모든 소프트맥스 변환을 합치면 그 결과는 항상 1이 되며, 항상 양수임

---
## The way to think about the softmax function

![19.Pasted image 20241002213553](../pic/5.%20Math,%20numpy,%20PyTorch/19.Pasted%20image%2020241002213553.png)

	인풋을 넣으면 소프트맥스는 총합이 1인 값을 반환한다.

![19.Pasted image 20241002213636](../pic/5.%20Math,%20numpy,%20PyTorch/19.Pasted%20image%2020241002213636.png)

	이 숫자들(thing1,thing2,thing3...)은 무엇인가?
	사실 어느정도 임의의 값임. 딥러닝 네트워크가 자체적으로 출력하는 이 숫자들은 사실 해석하기 어려움
	이 숫자들을 직접적으로 해석할 수는 없음.

	딥러닝 모델이 출력한 이 숫자들의 집합을, 소프트맥스 함수에 넣어주고, 그 결과로 확률 값의 집합을 얻음
	ex)이미지가 고양이일 확률 .9, 개일 확률 .01, 자동차일 확률 .001 

모델이 특정 카테고리에 속할 확률을 제공함

---
## Softmax and input/output ranges

![19.Pasted image 20241002215214](../pic/5.%20Math,%20numpy,%20PyTorch/19.Pasted%20image%2020241002215214.png)

	소프트맥스 함수를 적용한후 변환된 값들
	왼쪽에서 오른쪽으로 갈 수록 N의 크기가 늘어남
	총합이 1이 되기 위해 N의 크기가 늘어날 수록 y의 크기는 작아진다.

<span style="color:rgb(118, 147, 234)">입력에 대한 합:</span> <span style="color:rgb(116, 195, 194)">임의의 수치 값</span>
<span style="color:rgb(205, 205, 81)">출력에 대한 합:</span> <span style="color:rgb(236, 158, 111)">반드시 1.0이 되어야 함</span> 

---
## Code

```python
import numpy as np
import torch
import torch.nn as nn # neural networks
import matplotlib.pyplot as plt
```

```python
# "manually" in numpy

# the list of numbers
z = [1,2,3]

# compute the softmax result
num = np.exp(z)
den = np.sum(np.exp(z))
sigma = num/den

print(sigma)
print(sum(sigma))
```
```
[0.09003057 0.24472847 0.66524096]
1.0
```

```python
# repeat with some random integers
z = np.random.randint(-5,high=15,size=25)
print(z)

# compute the softmax result
num = np.exp(z)
den = np.sum(num)
sigma= num / den

# compare
plt.plot(z,sigma,'ko')
plt.xlabel('Original number (z)')
plt.ylabel('Softmaxified $\sigma$')
#plt.yscale('log') #자연지수함수의 역함수는 로그함수이기떄문에, 그래프가 완전한 선형으로 변함
plt.title('$\sum\sigma$ = %g'%np.sum(sigma))
plt.show()
```
```
[12 -3  8 -1 11  1 12  1  4  6  2  9  0 -1  7 -1 -3 12  0  0  3 13 11 -1
 -4]
```
![19.Pasted image 20241002222718](../pic/5.%20Math,%20numpy,%20PyTorch/19.Pasted%20image%2020241002222718.png)

```python
#### Using pytorch

# slightly more involved using torch.nn

# create an instance of the sftmax activtion class
softfun = nn.Softmax(dim=0)

# then apply the data to that function
sigmaT = softfun(torch.Tensor(z))

# now we get the results
print(sigmaT)
```
```
tensor([1.5309e-01, 4.6832e-08, 2.8040e-03, 3.4604e-07, 5.6320e-02, 2.5569e-06,
        1.5309e-01, 2.5569e-06, 5.1357e-05, 3.7948e-04, 6.9505e-06, 7.6221e-03,
        9.4064e-07, 3.4604e-07, 1.0315e-03, 3.4604e-07, 4.6832e-08, 1.5309e-01,
        9.4064e-07, 9.4064e-07, 1.8893e-05, 4.1615e-01, 5.6320e-02, 3.4604e-07,
        1.7228e-08])
```

```python
# show that they are the same
plt.plot(sigma,sigmaT,'ko')
plt.xlabel('"Manual" softmax')
plt.ylabel('Pytorch nn.Softmax')
plt.title(f'The two methods correlate at r={np.corrcoef(sigma,sigmaT)[0,1]}')
plt.show()
```
![19.Pasted image 20241002222758](../pic/5.%20Math,%20numpy,%20PyTorch/19.Pasted%20image%2020241002222758.png)
