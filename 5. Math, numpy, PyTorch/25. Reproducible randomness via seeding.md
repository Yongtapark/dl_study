- numpy와 pytorch의 시드 functions를 사용하는 방법
- 파이썬엔 다양한 시드들이 존재하고, 어떤 시드가 설정되었으며, 그 적용 범위가 무엇인지 신경써야 함

현실에서는 무작위성을 통제하는것이 거의 불가능하다. 하지만 컴퓨터에서는 시딩(seeding)이라는 절차를 통해 가능하다

왜 우리는 무작위성을 통제해야 할까? 

	딥러닝 모델을 초기화할 때, 모델에 초기 랜덤 상태를 제공한다. 즉, 무작위 가중치로 시작하게 하는 것.
	모델을 만들 때 마다 학습이 새로운 무작위 지점에서 시작된다는 것을 의미

	일반적으로 모델이 지역 최소값에 갇히는 것을 방지하는데 도움이 되기 때문에 좋음
	하지만 다른 사람이 모델을 재현 할 수 없을 수 도 있다는 점에서 불리할 수 있음

---
## Code

```python
# generate a few ranodm numbers
np.random.randn(5)
```
```
array([-1.20110842, -0.32361322,  1.32364325,  1.58143646,  1.59889736])
```

```python
# repeat after fixing the seed (old-but-still-widely-used method)
np.random.seed(17)
print(np.random.randn(5))
print(np.random.randn(5))

# [ 0.27626589 -1.85462808  0.62390111  1.14531129  1.03719047]
# [ 1.88663893 -0.11169829 -0.36210134  0.14867505 -0.43778315]
```
```
[ 0.27626589 -1.85462808  0.62390111  1.14531129  1.03719047]
[ 1.88663893 -0.11169829 -0.36210134  0.14867505 -0.43778315]
```

## New seed mechanism in numpy

```python
randseed1 = np.random.RandomState(17)
randseed2 = np.random.RandomState(20210530)

print(randseed1.randn(5)) # same sequence 
print(randseed2.randn(5)) # different from above, but same time
print(randseed1.randn(5)) # same as two up
print(randseed1.randn(5)) # same as two up
# 최근 function의 경우 변수화된 랜덤값은 고정되지만, 그렇지 않은 경우는 고정되지 않음
print(np.random.randn(5)) # different every time 
```
```
[ 0.27626589 -1.85462808  0.62390111  1.14531129  1.03719047]
[-0.24972681 -1.01951826  2.23461339  0.72764703  1.2921122 ]
[ 1.88663893 -0.11169829 -0.36210134  0.14867505 -0.43778315]
[ 2.171257    1.15231025 -1.81881234 -0.13804934  0.53983961]
[-0.9822943   1.03126909  0.49133378 -0.4466466  -0.80636008]
```

## Now in pytorch

```python
torch.randn(5)
```
```
tensor([-0.6124, -1.1835, -1.4831,  1.8004,  0.0096])
```

```python
torch.manual_seed(17)
print(torch.randn(5))

# torch's seed doesn't spread to numpy
print(np.random.randn(5))
```
```
tensor([-1.4135,  0.2336,  0.0340,  0.3499, -0.0145])
[ 0.86962384  0.56778309  0.46528234 -1.16537308 -2.03599479]
```
